{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7fb5ddf-9609-466b-9daf-88a8062cbcbc",
   "metadata": {},
   "source": [
    "# **SIADS Milestone I - Sentiment Analysis of TikTok Video**\n",
    "\n",
    "- **Cydia Tsang (cydia@umich.edu)**, School of Information, University of Michigan\n",
    "- **Yao Tong (tonyao@umich.edu)**, School of Information, University of Michigan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ee5fb4-aabc-42ef-8d50-7b78980658b4",
   "metadata": {},
   "source": [
    "#### **Strcuture of the Code:**\n",
    "\n",
    "&emsp; **Data Import**<br>\n",
    "&emsp;&emsp;&emsp; 1. Primary Dataset - trending.json<br>\n",
    "&emsp;&emsp;&emsp; 2. Seconndary Dataset - audd_music.csv<br>\n",
    "&emsp;&emsp;&emsp; 3. Thrid Dataset - audd_music_apple_music.csv<br>\n",
    "&emsp; **Data Cleaning & Manipulation**<br>\n",
    "&emsp;&emsp;&emsp; 1. Basic Desciptive Statistic Data Manipulation<br>\n",
    "&emsp;&emsp;&emsp; 2. Music Related Data Manipulation<br>\n",
    "&emsp;&emsp;&emsp; 3. Author Related Data Manipulation<br>\n",
    "&emsp;&emsp;&emsp; 4. Video Related Data Manipulation<br>\n",
    "&emsp;&emsp;&emsp; 5. Test Related Data Manipulation<br>\n",
    "&emsp; **Data Analysis & Visualisation**<br>\n",
    "&emsp;&emsp;&emsp; 1. Basic Desciptive Statistic Analysis<br>\n",
    "&emsp;&emsp;&emsp; 2. Music Related Analysis<br>\n",
    "&emsp;&emsp;&emsp; 3. Author Related Analysis<br>\n",
    "&emsp;&emsp;&emsp; 4. Video Related Analysis<br>\n",
    "&emsp;&emsp;&emsp; 5. Test Related Analysis<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9461aec-9e8e-4f9a-898d-225e9890c3c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidToken",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidSignature\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/IdeaProjects/untitled/venv/lib/python3.8/site-packages/cryptography/fernet.py:130\u001b[0m, in \u001b[0;36mFernet._verify_signature\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidSignature:\n",
      "\u001b[0;31mInvalidSignature\u001b[0m: Signature did not match digest.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidToken\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mboto3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamodb\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypeDeserializer\n\u001b[1;32m     22\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mgetcwd() \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/src\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpj_config\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamo_db_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_dynamo_db_client\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvaderSentiment\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvaderSentiment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentimentIntensityAnalyzer\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdecimal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Decimal\n",
      "File \u001b[0;32m~/IdeaProjects/04-tonyao-cydia-2024/src/pj_config/dynamo_db_client.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maws_access_key.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     15\u001b[0m     aws_access_key_encrypted_string \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcipher_suite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecrypt\u001b[49m\u001b[43m(\u001b[49m\u001b[43maws_access_key_encrypted_string\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dynamo_db_client\u001b[39m():\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maws_access_key.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
      "File \u001b[0;32m~/IdeaProjects/untitled/venv/lib/python3.8/site-packages/cryptography/fernet.py:89\u001b[0m, in \u001b[0;36mFernet.decrypt\u001b[0;34m(self, token, ttl)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     time_info \u001b[38;5;241m=\u001b[39m (ttl, \u001b[38;5;28mint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime()))\n\u001b[0;32m---> 89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decrypt_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_info\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/IdeaProjects/untitled/venv/lib/python3.8/site-packages/cryptography/fernet.py:148\u001b[0m, in \u001b[0;36mFernet._decrypt_data\u001b[0;34m(self, data, timestamp, time_info)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current_time \u001b[38;5;241m+\u001b[39m _MAX_CLOCK_SKEW \u001b[38;5;241m<\u001b[39m timestamp:\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidToken\n\u001b[0;32m--> 148\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_verify_signature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m iv \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m9\u001b[39m:\u001b[38;5;241m25\u001b[39m]\n\u001b[1;32m    151\u001b[0m ciphertext \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m25\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m32\u001b[39m]\n",
      "File \u001b[0;32m~/IdeaProjects/untitled/venv/lib/python3.8/site-packages/cryptography/fernet.py:132\u001b[0m, in \u001b[0;36mFernet._verify_signature\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    130\u001b[0m     h\u001b[38;5;241m.\u001b[39mverify(data[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m32\u001b[39m:])\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidSignature:\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidToken\n",
      "\u001b[0;31mInvalidToken\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import json\n",
    "import ast\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from wordcloud import WordCloud\n",
    "import itertools\n",
    "import nltk\n",
    "from nltk.probability import FreqDist, ConditionalFreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from scipy.stats import pearsonr\n",
    "from boto3.dynamodb.types import TypeDeserializer\n",
    "sys.path.append(os.getcwd() + \"/src\")\n",
    "from src.pj_config.dynamo_db_client import get_dynamo_db_client\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from decimal import Decimal\n",
    "from collections import Counter\n",
    "plt.rcParams['font.family'] = 'Arial Unicode MS'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90705ce1-2a78-46cc-a743-d51091895e55",
   "metadata": {},
   "source": [
    "# **Data Import**\n",
    "\n",
    "The trending.json was initially imported into DynamoDB and retrived by the scan() method from boto3.\n",
    "\n",
    "The following code aims to convert a response from a DynamoDB scan operation into a Pandas DataFrame. Initially, it retrieves items from the DynamoDB table 'tiktok_trending' using the scan operation. To handle cases where the result spans multiple pages, it iterates through each page, continuously appending the retrieved items.\n",
    "\n",
    "The convert_dynamodb_response function plays a crucial role in converting the raw DynamoDB response into a DataFrame. It first defines a deserializer to handle data types, then parses each item in the response to convert it into a dictionary. This dictionary comprehension ensures that the data is deserialized appropriately. Subsequently, any dictionary values are further processed to extract their string representations. Finally, these processed items are converted into a DataFrame, providing a structured representation of the DynamoDB data suitable for analysis and manipulation in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb1181a-3069-4a95-9ac7-09ae8202ce74",
   "metadata": {},
   "source": [
    "### **1. Primary Dataset - trending.json**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05332f75-d5cb-41c3-9e33-be792ebd8494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dynamodb_response(response):\n",
    "    deserializer = TypeDeserializer()\n",
    "    def parse_item(item):\n",
    "        return {key: deserializer.deserialize(value) for key, value in item.items()}\n",
    "  \n",
    "    items_temp = [parse_item(item) for item in items]\n",
    "    df = pd.DataFrame(items_temp)\n",
    "    df = df.applymap(lambda x: x if not isinstance(x, dict) else x.get('S', x))\n",
    "    return df\n",
    "    \n",
    "dynamodb = get_dynamo_db_client()\n",
    "response = dynamodb.scan(TableName='tiktok_trending')\n",
    "\n",
    "items=response[\"Items\"]\n",
    "\n",
    "while 'LastEvaluatedKey' in response and response['LastEvaluatedKey'] != \"\":  \n",
    "    response = dynamodb.scan(TableName='tiktok_trending', ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "    items.extend(response[\"Items\"])\n",
    "df = convert_dynamodb_response(items)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f011a7a4-b740-4524-aed3-6b52a320d857",
   "metadata": {},
   "source": [
    "### **2. Secondary Dataset - audd_music.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4882ecec-0338-4ef7-abfe-258cfaeddbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audd_music_df = pd.read_csv(\n",
    "    'src/data/audd/audd_music.csv',\n",
    "    sep=',',  # Specify the delimiter if it's not a comma\n",
    "    skiprows=1,  # Skip rows if necessary (e.g., skip the first row)\n",
    "    names=['id', 'artist', 'title','album','release_data','label','timecode','song_link','apple_music.isrc','spotify.id'],  \n",
    "    encoding='utf-8'  # Specify the encoding if there are special characters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d3460-4abc-431f-ac3d-32664196d40d",
   "metadata": {},
   "source": [
    "### **3. Third Dataset - audd_music_apple_music.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698ca3b3-13be-4eb4-91d5-affd63fb72d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "audd_apple_music_df = pd.read_csv(\n",
    "    'src/data/audd/audd_music_apple_music.csv',\n",
    "    sep=',',  # Specify the delimiter if it's not a comma\n",
    "    skiprows=1,  # Skip rows if necessary (e.g., skip the first row)\n",
    "    names=['isrc', 'artistName', 'url', 'discNumber', 'genreNames',\n",
    "       'durationInMillis', 'releaseDate', 'name', 'albumName', 'trackNumber',\n",
    "       'composerName', 'artwork.width', 'artwork.height', 'artwork.url',\n",
    "       'artwork.bgColor', 'artwork.textColor1', 'artwork.textColor2',\n",
    "       'artwork.textColor3', 'artwork.textColor4', 'playParams.id',\n",
    "       'playParams.kind'],  \n",
    "    encoding='utf-8'  # Specify the encoding if there are special characters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b3b48-f4ec-466c-b696-10e07782dbdd",
   "metadata": {},
   "source": [
    "# **Data Cleaning & Manipulation**\n",
    "The part encompasses several data preprocessing steps commonly encountered in exploratory data analysis (EDA) tasks, primarily focusing on structuring data extracted from the TikTok platform for subsequent analysis.\n",
    "\n",
    "Firstly, it performs basic data preprocessing, including the conversion of specified columns to numeric types and the removal of outliers based on the interquartile range (IQR) method. Subsequently, the code segments the dataset into four distinct categories: user, music, video, and text, each tailored to the nature of the features they encapsulate.\n",
    "\n",
    "For user-related data, it involves extracting pertinent information about TikTok authors and restructuring it for clarity and ease of analysis. Similarly, for music-related data, it involves processing music metadata and merging it with external data from the AuDD platform, facilitating comprehensive insights into the music used in TikTok content. Video data undergoes a transformation to extract key attributes such as width, duration, and height from the video metadata. Lastly, text-related data are structured to include relevant hashtag information and categorize various engagement metrics into discrete categories.\n",
    "\n",
    "By segmenting and preprocessing the TikTok dataset into these distinct categories, the code lays the groundwork for subsequent analysis, enabling researchers and analysts to delve deeper into specific aspects of TikTok content and user engagement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadf9be0-3c60-49bb-8640-b308cb8a2889",
   "metadata": {},
   "source": [
    "### **1. Basic Descriptive Statistic Data Manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8fc91d-7fb4-4668-9346-8f20e9017532",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Basic\n",
    "                          \n",
    "df = df.copy()\n",
    "numeric_columns = ['diggCount', 'commentCount', 'shareCount', 'playCount']\n",
    "df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "Q1 = df[numeric_columns].quantile(0.05)\n",
    "Q3 = df[numeric_columns].quantile(0.80)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "def remove_outliers(df):\n",
    "    return df[~((df[numeric_columns] < (Q1 - 1.5 * IQR)) | (df[numeric_columns] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    \n",
    "df = remove_outliers(df)\n",
    "\n",
    "basic_df = df[['id', 'createTime', 'diggCount', 'commentCount', 'shareCount', 'playCount']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67ade94-cb78-40b2-869c-c8910b9c71d1",
   "metadata": {},
   "source": [
    "### **2. Music Related Data Manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8104096c-c68b-480b-89c7-6807f0f655c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Music\n",
    "merged_music_df = pd.merge(audd_music_df, audd_apple_music_df, left_on='apple_music.isrc', right_on='isrc', how='inner')\n",
    "final_music_df = merged_music_df[['id', 'label', 'timecode', 'genreNames', 'releaseDate']]\n",
    "final_music_df = final_music_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec2974d-fb44-4bfb-bd71-7780ee0238d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "music_df = df[['id', 'createTime', 'musicMeta', 'diggCount', 'commentCount', 'shareCount', 'playCount', 'downloaded', 'text', 'hashtags']]\n",
    "music_df = music_df.copy()\n",
    "music_df['musicMeta'] = music_df['musicMeta'].apply(lambda x: {\n",
    "    'musicId': x['musicId'],\n",
    "    'musicAuthor': x['musicAuthor'],\n",
    "    'musicName': x['musicName'],\n",
    "    'musicOriginal': x['musicOriginal'],\n",
    "    'playUrl': x['playUrl']\n",
    "})\n",
    "music_df[['musicId', 'musicAuthor', 'musicName', 'musicOriginal', 'playUrl']] = pd.DataFrame(music_df['musicMeta'].tolist())\n",
    "music_df.drop(columns=['musicMeta'], inplace=True)\n",
    "music_df = music_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8df13d2-9220-49f9-9099-1f5fb4cba6ef",
   "metadata": {},
   "source": [
    "### **3. Author Related Data Manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63b64b-1121-4395-8409-c674d96cc056",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### User\n",
    "authors_df = df[['id', 'createTime', 'authorMeta', 'diggCount', 'commentCount', 'shareCount', 'playCount', 'downloaded']]\n",
    "authors_df = authors_df.copy()\n",
    "authors_df['authorMeta'] = authors_df['authorMeta'].apply(lambda x: {'secUid': x['secUid'], \n",
    "                                                               'signature': x['signature'], \n",
    "                                                               'nickName': x['nickName'], \n",
    "                                                               'name': x['name'], \n",
    "                                                               'verified': x['verified'], \n",
    "                                                               'id': x['id'], \n",
    "                                                               'avatar': x['avatar']})\n",
    "\n",
    "authors_df[['secUid', 'signature', 'nickName', 'name', 'verified', 'id', 'avatar']] = pd.DataFrame(authors_df['authorMeta'].tolist())\n",
    "authors_df = authors_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3deddc-fb82-4b24-9472-5b0ceb776c3e",
   "metadata": {},
   "source": [
    "### **4. Video Related Data Manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1cc3b-83d8-416d-b9c5-6a5ad349d59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Video\n",
    "video_df = df[['id', 'createTime', 'videoMeta', 'diggCount', 'commentCount', 'shareCount', 'playCount', 'downloaded']]\n",
    "video_df = video_df.copy()\n",
    "\n",
    "# Extract videoMeta into 3 fields(video_width, video_duration and video_height)\n",
    "video_df['videoMeta'] = video_df['videoMeta'].apply(lambda x: {'width': int(x['width']), 'duration': int(x['duration']), 'height': int(x['height'])})\n",
    "video_df[['video_width', 'video_duration', 'video_height']] = pd.DataFrame(video_df['videoMeta'].tolist())\n",
    "\n",
    "video_df = video_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cd0b84-ba33-47a8-9b50-9e928e18df6a",
   "metadata": {},
   "source": [
    "### **5. Text Related Data Manipulation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7a339b-8022-4c15-a1a5-53e9fded9c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Text\n",
    "text_df = df[['id', 'createTime', 'hashtags', 'mentions', 'diggCount', 'commentCount', 'shareCount', 'playCount', 'downloaded']]\n",
    "\n",
    "categories = ['diggCountCategory', 'shareCountCategory', 'playCountCategory', 'commentCountCategory']\n",
    "target_values = ['very low', 'low', 'medium', 'high', 'very high']\n",
    "\n",
    "# Divided diggCount, commentCount, shareCount and playCount into 'very low', 'low', 'medium', 'high', 'very high' 5 catagories\n",
    "def process_numeric_column(df, column_name):\n",
    "    df_copy = df.copy()\n",
    "    quantiles = df_copy[column_name].quantile([0, 1/5, 2/5, 3/5, 4/5, 1])\n",
    "    df_copy[f'{column_name}Category'] = pd.cut(df_copy[column_name], bins=quantiles, labels=target_values, include_lowest=True)\n",
    "    return df_copy\n",
    "\n",
    "text_df = process_numeric_column(text_df, 'diggCount')\n",
    "text_df = process_numeric_column(text_df, 'commentCount')\n",
    "text_df = process_numeric_column(text_df, 'shareCount')\n",
    "text_df = process_numeric_column(text_df, 'playCount')\n",
    "\n",
    "text_df['hashtag'] = text_df['hashtags'].apply(lambda x: [item['name'] for item in x] if x else [])\n",
    "text_df = text_df.explode('hashtag').reset_index(drop=True)\n",
    "text_df = text_df[['id', 'createTime', 'hashtag', 'diggCount', 'commentCount', 'shareCount', 'playCount','diggCountCategory', 'shareCountCategory', 'playCountCategory', 'commentCountCategory']]\n",
    "text_df = text_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f35bbc9-467b-40b8-8662-f9b2aa27c0e1",
   "metadata": {},
   "source": [
    "# **Data Analysis & Visualisation**\n",
    "This part will cover the data analysis and visualization pipeline covering various domains such as music, video, and text. Let's break down each section and understand its purpose and what insights can be derived from the visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e9d1ac-f7f4-4c2f-8a8b-40958c962d83",
   "metadata": {},
   "source": [
    "### **1. Basic Descriptive Statistic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bb036f-ddac-4f47-b8c9-e87a41fe0c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Basic\n",
    "descriptive_stats = basic_df[['diggCount', 'commentCount', 'shareCount', 'playCount']].describe()\n",
    "\n",
    "print(\"Descriptive Statistics:\")\n",
    "print(descriptive_stats)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "colors = ['skyblue', 'lightcoral', 'lightgreen', 'lightyellow']\n",
    "for i, (col, color) in enumerate(zip(numeric_columns, colors), 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.boxplot(x=basic_df[col], color=color)\n",
    "    plt.title(f'Box Plot for {col}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78d9b35-9f63-4318-a34f-2fc3fbd28d6f",
   "metadata": {},
   "source": [
    "### **2. Music Related Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ec11e0-d8f3-40be-883c-8497a0aba1e6",
   "metadata": {},
   "source": [
    "Timecode in TikTok's musicMeta typically refers to the specific timestamp or point in the background music track from which the music starts playing in the video.\n",
    "For the analysis of music, we selected the Timecode of the background music based on the top trending videos as the main clue to try to analyze the music pattern preferred by trending videos and analyze the sentiment of the music and video text. In descriptive data, the timecodes used by different music genres are distinguished, but the commonly used timecodes are concentrated around 50s of a song. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ab5b24-8202-4890-b5a0-f0617314593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_music_df['genreNames'] = final_music_df['genreNames'].apply(lambda x: str(x) if not isinstance(x, str) else x)\n",
    "final_music_df['genreNames'] = final_music_df['genreNames'].apply(ast.literal_eval)\n",
    "all_genres = [genre for sublist in final_music_df['genreNames'] for genre in sublist if genre != 'Music']\n",
    "genre_counts = Counter(all_genres)\n",
    "genre_counts_df = pd.DataFrame(genre_counts.items(), columns=['Genre', 'Count'])\n",
    "\n",
    "exploded_df = final_music_df.explode('genreNames')\n",
    "exploded_df = exploded_df[exploded_df['genreNames'] != 'Music']\n",
    "\n",
    "def timecode_to_seconds(timecode):\n",
    "    # Split the timecode into minutes and seconds\n",
    "    minutes, seconds = map(int, timecode.split(':'))\n",
    "    # Convert the timecode to total seconds\n",
    "    return minutes * 60 + seconds\n",
    "\n",
    "# Apply the conversion function to each timecode\n",
    "exploded_df['timecode'] = exploded_df['timecode'].apply(timecode_to_seconds)\n",
    "\n",
    "# Combine median and mean timecodes into a single DataFrame\n",
    "top_10_genres = genre_counts_df.nlargest(10, 'Count')['Genre']\n",
    "\n",
    "# Step 2: Filter 'exploded_df' to include only the top 10 genres\n",
    "filtered_df = exploded_df[exploded_df['genreNames'].isin(top_10_genres)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accf5cde-4284-42d7-8bc4-0f3100bbaf02",
   "metadata": {},
   "source": [
    "The follwoing displot is the top 10 genres based on their timecodes, using seaborn. It then calculates and overlays lines representing the global median and mean timecodes across all top 10 genres on the plot, enhancing the visualization to compare individual genre distributions with global statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b947a9e3-c1c1-4418-ae1c-5a8afcd9f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a displot for the top 10 genres\n",
    "g = sns.displot(\n",
    "    data=filtered_df,\n",
    "    x='timecode',\n",
    "    hue='genreNames',\n",
    "    kind='kde',\n",
    "    height=6, aspect=1.5,\n",
    "    palette='tab20',\n",
    "    common_norm=False,\n",
    "    facet_kws={'legend_out': True}\n",
    ")\n",
    "\n",
    "# Calculate the global median and mean timecode across all top 10 genres\n",
    "global_median = filtered_df['timecode'].median()\n",
    "global_mean = filtered_df['timecode'].mean()\n",
    "\n",
    "# Add lines for global median and mean timecodes\n",
    "plt.axvline(global_median, color='grey', linestyle='--', linewidth=1, label='Global Median')\n",
    "plt.axvline(global_mean, color='black', linestyle='-', linewidth=1, label='Global Mean')\n",
    "\n",
    "# Create custom legend handles\n",
    "median_line = plt.Line2D([], [], color='grey', linestyle='--', linewidth=2, label='Global Median')\n",
    "mean_line = plt.Line2D([], [], color='grey', linestyle='-', linewidth=2, label='Global Mean')\n",
    "\n",
    "# Add the custom legend with the original legend\n",
    "#plt.legend(handles=g._legend.legendHandles + [median_line, mean_line], bbox_to_anchor=(1.05, 1), loc=2)\n",
    "\n",
    "plt.title('Distribution of Timecodes by Top 10 Genres with Median and Mean Lines')\n",
    "plt.xlabel('Timecode (seconds)')\n",
    "plt.ylabel('Density')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce557d14-9a59-4b8f-b902-85ac67d8947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To install, use !pip install vaderSentiment\n",
    "# Initialize VADER sentiment analyzer\n",
    "exploded_df = exploded_df.rename(columns={'id': 'musicId'})\n",
    "music_df['musicId'] = music_df['musicId'].astype(int)\n",
    "merged_df = pd.merge(music_df, exploded_df, left_on='musicId', right_on='musicId', how='left')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to clean text data\n",
    "def clean_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^A-Za-z\\s]+', '', text)\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "merged_df['text'] = merged_df['text'].apply(clean_text)\n",
    "merged_df['sentiment_score'] = merged_df['text'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd38ef3b-be84-486a-a9a1-f23ae13e9d98",
   "metadata": {},
   "source": [
    "#### Explanation about the sentiment_score:\n",
    "High Score (Close to 1)\n",
    "A high sentiment score, approaching 1, usually indicates a strong positive sentiment. Texts with high scores are likely to contain positive expressions, praising comments, or optimistic language. For instance, phrases like \"absolutely fantastic\", \"incredibly happy\", or \"best experience ever\" would contribute to a high sentiment score.\n",
    "\n",
    "Low Score (Close to -1)\n",
    "A low sentiment score, approaching -1, suggests a strong negative sentiment. This means the text likely includes criticisms, expressions of dissatisfaction, sadness, or anger. Examples of phrases that might result in a low sentiment score include \"terribly disappointed\", \"worst service\", or \"deeply saddened\".\n",
    "\n",
    "Score Around Zero\n",
    "Sentiment scores close to 0 can indicate neutral sentiment, where the text might be factual, lack emotional language, or contain an equal mix of positive and negative sentiments that balance each other out. Neutral texts might include factual statements like \"The conference starts at 9 AM\" or mixed sentiment expressions like \"I love the camera on this phone, but the battery life is disappointing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6185d638-70cb-41ae-bebb-c1a50ef68fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_median_scores = merged_df.groupby('genreNames')['sentiment_score'].median().sort_values()\n",
    "\n",
    "ordered_genres = genre_median_scores.index\n",
    "#Create a box plot with ordered genres\n",
    "plt.figure(figsize=(14, 14))\n",
    "sns.boxplot(data=merged_df, x='sentiment_score', y='genreNames', hue='genreNames', order=ordered_genres, palette=\"coolwarm\", legend=False)\n",
    "\n",
    "# Increase separation between boxes for clarity\n",
    "sns.despine(offset=10, trim=True)\n",
    "\n",
    "plt.title('Ordered Distribution of Text Sentiment Scores Among Music Genres They Used')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Music Genre')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5aadc-da6f-4dad-8a78-f58bdeb25422",
   "metadata": {},
   "source": [
    "### **3. Author Related Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b0d278-ce58-4d0d-9126-a88ff07fa96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "verified_counts = authors_df['verified'].value_counts()\n",
    "\n",
    "# Create a bar chart\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=verified_counts.index.astype(str), y=verified_counts.values)  # Convert index to string for clarity\n",
    "\n",
    "plt.title('Distribution of Verified Status')\n",
    "plt.xlabel('Verified Status')\n",
    "plt.ylabel('Count')\n",
    "# Dynamically set x-tick labels based on the order in verified_counts\n",
    "plt.xticks([0, 1], [f'{verified_counts.index[0]} (Not Verified)', f'{verified_counts.index[1]} (Verified)'])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "authors_df = authors_df.rename(columns={'id': 'authors_id'})\n",
    "combined_df = pd.concat([authors_df, merged_music_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dedddc-4835-470a-a469-da080a339c97",
   "metadata": {},
   "source": [
    "### **4. Video Related Analysis**\n",
    "\n",
    "This section computes and displays the correlation coefficient between the duration of videos and various engagement metrics such as diggCount, shareCount, playCount, and commentCount. It will then fit a linear regression model to predict video duration based on engagement metrics and evaluates its performance using R-squared.The coefficients of the model are displayed, indicating the relative importance of each engagement metric in predicting video duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9f7156-ed16-49f0-9c77-963f44e22bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Video\n",
    "def correlation_analysis(column_name):\n",
    "    correlation_coefficient, _ = pearsonr(video_df['video_duration'], video_df[column_name])\n",
    "    return correlation_coefficient\n",
    "\n",
    "variables_to_compare = ['diggCount', 'shareCount', 'playCount', 'commentCount']\n",
    "\n",
    "print(\"Correlation Coefficient between video_duration and Calculated by pearsonr:\")\n",
    "# Compute and print correlation coefficients\n",
    "for variable in variables_to_compare:\n",
    "    correlation_coefficient = correlation_analysis(variable)\n",
    "    print(f\"Correlation Coefficient (video_duration vs {variable}): {correlation_coefficient}\")\n",
    "\n",
    "X = video_df[['diggCount', 'shareCount', 'playCount', 'commentCount']]\n",
    "y = video_df['video_duration']\n",
    "scaler = MinMaxScaler()\n",
    "X_normalized = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_normalized, y, test_size=0.2, random_state=42)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Coefficients: {model.coef_}\")\n",
    "print(f\"Intercept: {model.intercept_}\")\n",
    "print(f\"R-squared: {metrics.r2_score(y_test, y_pred)}\")\n",
    "print(f\"P-values: {model.coef_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dcbcc8-95b2-4912-aa2c-4bb7a0402ff3",
   "metadata": {},
   "source": [
    "This scatter plots visualize the relationship between video duration and each engagement metric, providing insights into how video length influences engagement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5619fd3-4af9-40b8-b920-50a3da1ee27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = [\"diggCount\", \"commentCount\", \"shareCount\", \"playCount\"]\n",
    "colors = sns.color_palette('husl', len(num_cols))\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i, col in enumerate(num_cols, start=1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.scatterplot(x=col, y='video_duration', data=video_df, color=colors[i-1])\n",
    "    plt.title(f'Scatter Plot: video_duration vs {col}', fontsize=16)\n",
    "    plt.xlabel(col, fontsize=14)\n",
    "    plt.ylabel('video_duration', fontsize=14) \n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eff104b-1e19-4794-99e0-a3d92509dcf4",
   "metadata": {},
   "source": [
    "This heatmap visualizes the correlation matrix between video duration and engagement metrics, offering a comprehensive view of their relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa79058d-94aa-470e-820b-81a38670b9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "video_new_df = video_df[[\"diggCount\", \"commentCount\", \"shareCount\", \"playCount\", \"video_duration\"]]\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = video_new_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a528cf37-4d12-4d49-9df5-c4ca5d873512",
   "metadata": {},
   "source": [
    "### **5. Text Related Analysis**\n",
    "\n",
    "This part calculates the probability distribution of hashtag categories (e.g., diggCountCategory, shareCountCategory) based on engagement counts. It identifies hashtags with very high and very low engagement counts within each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a3246-55be-474b-aa1b-4436e02c1dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Text\n",
    "def calculate_probability(text_df, count, category):\n",
    "    text_df = text_df[['hashtag', count, category]].sort_values(by='hashtag')\n",
    "\n",
    "    # Group the data by hashtag and category\n",
    "    group_text_df = text_df.groupby(['hashtag', category]).size().reset_index(name='count')\n",
    "    total_count_per_hashtag = text_df.groupby(['hashtag', category])[count].mean().reset_index(name='mean_' + count)\n",
    "    merged_df = pd.merge(group_text_df, total_count_per_hashtag, on=['hashtag', category])\n",
    "\n",
    "    # The probaility of 'very low', 'low', 'medium', 'high', 'very high' over total number of catagory\n",
    "    merged_df['probability'] = merged_df.groupby('hashtag')['count'].transform(lambda x: x / x.sum())\n",
    "    \n",
    "    # Filter out records that have more than 1 occuring and > 0.5 probability of each catagory over total cattagory\n",
    "    filtered_text_df = merged_df[(merged_df['count'] > 1) & (merged_df['probability'] > 0.5)]\n",
    "    \n",
    "    text_very_high_df = filtered_text_df[filtered_text_df[category] == 'very high']\n",
    "    text_very_low_df = filtered_text_df[filtered_text_df[category] == 'very low']\n",
    "    return text_df, text_very_high_df.head(10), text_very_low_df.head(10)\n",
    "\n",
    "digg_count_cat_df, digg_count_cat_very_high, digg_count_cat_very_low = calculate_probability(text_df, 'diggCount', 'diggCountCategory')\n",
    "share_count_cat_df, share_count_cat_very_high, share_count_cat_very_low = calculate_probability(text_df, 'shareCount', 'shareCountCategory')\n",
    "play_count_cat_df, play_count_cat_very_high, play_count_cat_very_low = calculate_probability(text_df, 'playCount', 'playCountCategory')\n",
    "comment_count_cat_df, comment_count_cat_very_high, comment_count_cat_very_low = calculate_probability(text_df, 'commentCount', 'commentCountCategory')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6c13f-4fac-4671-b764-f029e2b6fba9",
   "metadata": {},
   "source": [
    "a word cloud visualizes the most frequent hashtags, providing insights into popular topics or themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8897eb1e-9656-4630-ab0d-d8abd2c398a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hashtags = ' '.join(text_df['hashtag'].dropna())\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_hashtags)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud for Hashtags')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ef6bcc-2651-4077-8c81-55453fa14473",
   "metadata": {},
   "source": [
    "These Bar charts showcase the top hashtags with very high engagement counts for each category, allowing for a deeper understanding of popular topics on the platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dd8ff0-6c6c-47c3-a0ed-2cd6f1799100",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getBarChart(df, mean_count, count_name, min_val, max_val, color='blue'):\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    bar_width = 0.10\n",
    "    ax.bar(df['hashtag'], df[mean_count], color=color, label=mean_count)\n",
    "    ax.set_xlabel('Hashtag', fontsize=14)\n",
    "    ax.set_ylabel(mean_count, fontsize=14)\n",
    "    ax.set_title('Top 15 Hashtags that have ' + count_name, fontsize=16)\n",
    "    ax.legend()\n",
    "    plt.xticks(fontsize=16)\n",
    "    num_digits = len(str(max_val))\n",
    "    nearest_multiple = 10 ** (num_digits - 4)\n",
    "    ax.set_ylim(math.floor(min_val), int(math.ceil(max_val / float(nearest_multiple))) * nearest_multiple)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "getBarChart(digg_count_cat_very_high, 'mean_diggCount', 'Very High diggCount', digg_count_cat_very_high['mean_diggCount'].min(), digg_count_cat_very_high['mean_diggCount'].max(), color='#f6a0a9')\n",
    "getBarChart(share_count_cat_very_high, 'mean_shareCount', 'Very High shareCount', share_count_cat_very_high['mean_shareCount'].min(), share_count_cat_very_high['mean_shareCount'].max(), color='#ddba99')\n",
    "getBarChart(play_count_cat_very_high, 'mean_playCount', 'Very High playCount', play_count_cat_very_high['mean_playCount'].min(), play_count_cat_very_high['mean_playCount'].max(), color='#dd99ba')\n",
    "getBarChart(comment_count_cat_very_high, 'mean_commentCount', 'Very High commentCount', comment_count_cat_very_high['mean_commentCount'].min(), comment_count_cat_very_high['mean_commentCount'].max(), color='#ea86e8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8677913-452a-4ca0-98f6-ef6475556553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef84ea6-5441-4527-8eb0-2022330b2fde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
